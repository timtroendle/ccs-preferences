import pymc as pm 
import pandas as pd
import arviz as az
import xarray as xr
import numpy as np

# %% pymc bug workaround

import pytensor
pytensor.config.cxx = '/usr/bin/clang++'

# %% 

df = pd.read_csv("data/hcm_input.csv")

# %% define attributes and baselines

attributes = [ 
    # "attr_engagement",
    "attr_vicinity",
    "attr_industry",
    "attr_costs",
    # "attr_reason",
    "attr_source_purpose"
]

baseline_dict = {
    # "attr_engagement": "inform",
    "attr_vicinity": "abroad",
    "attr_industry": "waste incineration",
    "attr_costs": "taxpayer",
    # "attr_reason": "sparsely-populated",
    "attr_source_purpose": "domestic"
}

# %% define dummies

# reorder each attribute column by making it categorical with the baseline first
for attr in attributes:
    baseline = baseline_dict[attr]
    df[attr] = pd.Categorical(df[attr], categories=[baseline] + 
                              [level for level in df[attr].unique() if level != baseline], 
                              ordered=True)

# generate dummies with columns in the correct order
dummies = pd.get_dummies(df[attributes], drop_first=False)

# reorder columns to place baseline first for each attribute
ordered_columns = []
for attr in attributes:
    # Collect the columns related to the attribute and put baseline first
    attr_columns = [col for col in dummies.columns if col.startswith(attr)]
    baseline_column = f"{attr}_{baseline_dict[attr]}"
    ordered_columns.append(baseline_column)
    ordered_columns.extend([col for col in attr_columns if col != baseline_column])

# reorder dummies according to ordered columns list
dummies = dummies[ordered_columns]
dummies = dummies.loc[:, ~dummies.columns.duplicated()]

df["framing"] = df["framing"].astype("category")
df["country"] = df["country"].astype("category")

# add dimensions
unique_individuals = df["id"].unique()
coords = {
    "level": dummies.columns.tolist(),
    "task": np.arange(len(df) // 2),
    "framing": df["framing"].cat.categories,
    "country": df["country"].cat.categories,
    "individual": unique_individuals
}

id_to_index = {id_: i for i, id_ in enumerate(unique_individuals)}
df["individual_idx"] = df["id"].map(id_to_index)
individual_idx = df.loc[df.package == 1, "individual_idx"].values

# %% check coords

print("Coords:")
for k, v in coords.items():
    print(f"{k}: {len(v)} items")

print("\nFirst 13 individual_idx:", individual_idx[:13])
print("Number of tasks:", len(individual_idx))
print("Number of unique individuals:", len(unique_individuals))

# %% define HCM

with pm.Model(coords=coords) as hcm_model:

    # latent trait priors per individual
    lreco_latent = pm.Normal("lreco_latent", mu=0, sigma=1, dims="individual")
    galtan_latent = pm.Normal("galtan_latent", mu=0, sigma=1, dims="individual")
    socio_ecol_latent = pm.Normal("socio_ecol_latent", mu=0, sigma=1, dims="individual")

    # noise in observed scores
    likert_sigma = 0.1

    # Observed variables as continuous (normalized to 0â€“1)
    pm.Normal("lreco_1", mu=lreco_latent[individual_idx], sigma=likert_sigma, 
              observed=df.loc[df.package == 1, "lreco_1"].values)

    pm.Normal("lreco_2", mu=lreco_latent[individual_idx], sigma=likert_sigma, 
              observed=df.loc[df.package == 1, "lreco_2"].values)

    pm.Normal("lreco_3", mu=lreco_latent[individual_idx], sigma=likert_sigma, 
              observed=df.loc[df.package == 1, "lreco_3"].values)

    pm.Normal("galtan_1", mu=galtan_latent[individual_idx], sigma=likert_sigma, 
              observed=df.loc[df.package == 1, "galtan_1"].values)

    pm.Normal("galtan_2", mu=galtan_latent[individual_idx], sigma=likert_sigma, 
              observed=df.loc[df.package == 1, "galtan_2"].values)

    pm.Normal("galtan_3", mu=galtan_latent[individual_idx], sigma=likert_sigma, 
              observed=df.loc[df.package == 1, "galtan_3"].values)

    pm.Normal("socio_ecological_1", mu=socio_ecol_latent[individual_idx], sigma=likert_sigma, 
              observed=df.loc[df.package == 1, "socio_ecological_1"].values)

    pm.Normal("socio_ecological_2", mu=socio_ecol_latent[individual_idx], sigma=likert_sigma, 
              observed=df.loc[df.package == 1, "socio_ecological_2"].values)

    pm.Normal("socio_ecological_3", mu=socio_ecol_latent[individual_idx], sigma=likert_sigma, 
              observed=df.loc[df.package == 1, "socio_ecological_3"].values)

    # get framing and country codes
    f = pm.Data("f", df.loc[df.package == 1, "framing"].cat.codes.values, dims="task")
    f = f.astype("float32")
    country_idx = df.loc[df.package == 1, "country"].cat.codes.values
    c = pm.Data("c", country_idx, dims = "task")

    # observed choices
    observed_choice_left = pm.Data(
        "observed_choice_left", 
        df.loc[df.package == 1, "chosen"].values, 
        dims="task"
    )

    # attribute dummies
    attribute_levels_left = pm.Data(
        "attribute_levels_left", 
        dummies[df.package == 1].values, 
        dims=["task", "level"]
    )

    attribute_levels_right = pm.Data(
        "attribute_levels_right", 
        dummies[df.package == 2].values, 
        dims=["task", "level"]
    )

    # gamma coefficients: how much each latent trait moderates framing effects
    theta_lreco = pm.Normal("theta_lreco", mu=0, sigma=1, dims="level")
    theta_galtan = pm.Normal("theta_galtan", mu=0, sigma=1, dims="level")
    theta_ecol = pm.Normal("theta_ecol", mu=0, sigma=1, dims="level")
    
    # choice model: main effects
    beta = pm.Normal("beta", mu=0, sigma=2, dims="level")

    # framing-specific shift
    delta = pm.Normal("delta", mu=0, sigma=1, dims="level")

    # country effect
    gamma = pm.Normal("gamma", mu=0, sigma=1, dims=["country", "level"])

    beta_modulated = (
        beta
        + delta * f[:, None]
        + gamma[c, :]
        + theta_lreco * lreco_latent[individual_idx][:, None]
        + theta_galtan * galtan_latent[individual_idx][:, None]
        + theta_ecol * socio_ecol_latent[individual_idx][:, None]
    )

    # get utilities
    utility_left = pm.Deterministic("utility_left", 
        pm.math.sum(attribute_levels_left * beta_modulated, axis=1), dims = "task")

    utility_right = pm.Deterministic("utility_right", 
        pm.math.sum(attribute_levels_right * beta_modulated, axis=1), dims = "task")

    # logit probability
    prob_choice_left = pm.Deterministic("probability_choice_left", 
        pm.math.exp(utility_left) / (pm.math.exp(utility_left) + pm.math.exp(utility_right)),
        dims="task"
    )

    # likelihood
    pm.Bernoulli("choice_distribution", p=prob_choice_left, observed = observed_choice_left)

# %% get priors

priors = pm.sample_prior_predictive(
    samples = 1000, 
    model = hcm_model, 
    random_seed = 42, 
)

# %% check priors

priors.prior
priors.prior.keys()
az.summary(
    priors,
    var_names=["beta", "gamma", "delta", "theta_lreco",]
)

# %% test model 

with hcm_model:
    approx = pm.fit(n=10000, method="advi")
    trace = approx.sample(1000)

# %% run model (3 to 5 hours)

# run model with MCMC with 1000 draws, 500 tune samples, and 4 chains on 6 cores
inference_data = pm.sample(
    model = hcm_model, 
    draws = 1000, 
    tune = 500, 
    chains = 4,
    cores = 6, 
    random_seed = 42, 
    return_inferencedata = True, 
    target_accept = 0.9
)

# %% diagnostics

az.summary(inference_data, var_names=[
    "beta",
    "delta",
    "gamma",
    "theta_lreco",
    "theta_galtan",
    "theta_ecol"
    ])

az.plot_trace(inference_data, var_names=[
    "beta",
    "delta",
    "gamma",
    "theta_lreco",
    "theta_galtan",
    "theta_ecol"
    ])

# az.plot_dist(inference_data, var_names=[
#     "beta",
#     "delta",
#     "theta_lreco",
#     "theta_galtan",
#     "theta_ecol"
#     ])

az.plot_forest(inference_data, var_names=["beta"], combined=True)
az.plot_forest(inference_data, var_names=["delta"], combined=True)
az.plot_forest(inference_data, var_names=["gamma"], combined=True)
az.plot_forest(inference_data, var_names=["theta_lreco"], combined=True)
az.plot_forest(inference_data, var_names=["theta_galtan"], combined=True)
az.plot_forest(inference_data, var_names=["theta_ecol"], combined=True)

# %% save to netcdf

inference_data.to_netcdf("output/inference_hybrid_choice.nc")

# %%
